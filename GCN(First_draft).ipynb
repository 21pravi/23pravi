{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ+fJXGoZRnC5JrCyETYWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21pravi/23pravi/blob/main/GCN(First_draft).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmuDNxKim7ke"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from scipy.sparse import csr_matrix\n",
        "from imblearn.combine import SMOTEENN  # Import SMOTEENN\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "import contractions\n",
        "import unidecode\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/data (1).csv')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Function to get wordnet pos tags\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Expand contractions\n",
        "    text = contractions.fix(text)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove emojis\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    # Remove accents\n",
        "    text = unidecode.unidecode(text)\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "    # Stem words\n",
        "    words = [porter.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "\n",
        "# Label sentiment based on ratings\n",
        "conditions = [(df['Rating'] > 5), (df['Rating'] <= 5)]\n",
        "values = [1, 0]\n",
        "df['Sentiment'] = np.select(conditions, values)\n",
        "df.drop(columns='Rating', axis=1, inplace=True)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Clean text in 'Review' column\n",
        "df['Review'] = df['Review'].apply(clean_text)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=10000)  # Increase the number of features\n",
        "X = vectorizer.fit_transform(df['Review'])\n",
        "y = df['Sentiment'].values\n",
        "\n",
        "# Apply SMOTEENN\n",
        "smote_enn = SMOTEENN(random_state=42)\n",
        "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "def create_data_list(X, y):\n",
        "    data_list = []\n",
        "    for i in range(len(y)):\n",
        "        coo = csr_matrix(X[i])  # Convert each sample to COO format\n",
        "        row_start = coo.indptr[0]\n",
        "        row_end = coo.indptr[-1]\n",
        "        x = torch.tensor(np.asarray(coo.todense()), dtype=torch.float)\n",
        "        # Create a dummy edge index (replace this with your actual graph connectivity)\n",
        "        edge_index = torch.tensor([[], []], dtype=torch.long)\n",
        "        y_val = torch.tensor([y[i]], dtype=torch.long)  # Wrap y[i] in a list to make it a single-element tensor\n",
        "        # Create a Data object and append it to the list\n",
        "        data = Data(x=x, edge_index=edge_index, y=y_val)\n",
        "        data_list.append(data)\n",
        "    return data_list\n",
        "\n",
        "train_data_list = create_data_list(X_train, y_train)\n",
        "test_data_list = create_data_list(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data_list, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data_list, batch_size=32, shuffle=False)\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)  # Add dropout layer\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = global_mean_pool(x, batch=torch.zeros(x.size(0), dtype=torch.long))  # Apply global pooling\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 64\n",
        "hidden_dim = 32  # Increase hidden dimension\n",
        "output_dim = 2\n",
        "\n",
        "model = GCN(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)  # Decrease learning rate and add weight decay\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, optimizer, criterion, epochs=100):  # Increase number of epochs\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for data in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            # Ensure the target is squeezed to match the shape of the model output\n",
        "            loss = criterion(out, data.y.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "train_model(model, train_loader, optimizer, criterion)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            out = model(data)\n",
        "            preds = out.argmax(dim=1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "    accuracy = accuracy_score(np.hstack(all_labels), np.hstack(all_preds))\n",
        "    report = classification_report(np.hstack(all_labels), np.hstack(all_preds))\n",
        "    return accuracy, report\n",
        "\n",
        "accuracy, report = evaluate_model(model, test_loader)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import Libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mq4kB5k6nSC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from scipy.sparse import csr_matrix\n",
        "from imblearn.combine import SMOTEENN  # Import SMOTEENN\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "import contractions\n",
        "import unidecode"
      ],
      "metadata": {
        "id": "XpHGjJBInQgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing"
      ],
      "metadata": {
        "id": "KVmC9onOnobF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/data (1).csv')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Function to get wordnet pos tags\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Expand contractions\n",
        "    text = contractions.fix(text)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove emojis\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    # Remove accents\n",
        "    text = unidecode.unidecode(text)\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "    # Stem words\n",
        "    words = [porter.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "\n",
        "# Label sentiment based on ratings\n",
        "conditions = [(df['Rating'] > 5), (df['Rating'] <= 5)]\n",
        "values = [1, 0]\n",
        "df['Sentiment'] = np.select(conditions, values)\n",
        "df.drop(columns='Rating', axis=1, inplace=True)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Clean text in 'Review' column\n",
        "df['Review'] = df['Review'].apply(clean_text)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=10000)  # Increase the number of features\n",
        "X = vectorizer.fit_transform(df['Review'])\n",
        "y = df['Sentiment'].values"
      ],
      "metadata": {
        "id": "PbZitXSknmdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "oWaMR1D4RQSj",
        "outputId": "8b7c003b-7ba2-43b5-c4fc-5467f632dd98",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_test_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1b8b6ef835dd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define GCN model"
      ],
      "metadata": {
        "id": "8YwRs4ffRTx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_list(X, y):\n",
        "    data_list = []\n",
        "    for i in range(len(y)):\n",
        "        coo = csr_matrix(X[i])  # Convert each sample to COO format\n",
        "        row_start = coo.indptr[0]\n",
        "        row_end = coo.indptr[-1]\n",
        "        x = torch.tensor(np.asarray(coo.todense()), dtype=torch.float)\n",
        "        # Create a dummy edge index (replace this with your actual graph connectivity)\n",
        "        edge_index = torch.tensor([[], []], dtype=torch.long)\n",
        "        y_val = torch.tensor([y[i]], dtype=torch.long)  # Wrap y[i] in a list to make it a single-element tensor\n",
        "        # Create a Data object and append it to the list\n",
        "        data = Data(x=x, edge_index=edge_index, y=y_val)\n",
        "        data_list.append(data)\n",
        "    return data_list\n",
        "\n",
        "train_data_list = create_data_list(X_train, y_train)\n",
        "test_data_list = create_data_list(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data_list, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data_list, batch_size=32, shuffle=False)\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)  # Add dropout layer\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = global_mean_pool(x, batch=torch.zeros(x.size(0), dtype=torch.long))  # Apply global pooling\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 64\n",
        "hidden_dim = 32 # Increase hidden dimension\n",
        "output_dim = 2\n",
        "\n",
        "model = GCN(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)  # Decrease learning rate and add weight decay\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, optimizer, criterion, epochs=50):  # Increase number of epochs\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for data in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            # Ensure the target is squeezed to match the shape of the model output\n",
        "            loss = criterion(out, data.y.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "train_model(model, train_loader, optimizer, criterion)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            out = model(data)\n",
        "            preds = out.argmax(dim=1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "    accuracy = accuracy_score(np.hstack(all_labels), np.hstack(all_preds))\n",
        "    report = classification_report(np.hstack(all_labels), np.hstack(all_preds))\n",
        "    return accuracy, report\n",
        "\n",
        "accuracy, report = evaluate_model(model, test_loader)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "4BIypZ6zRRfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}